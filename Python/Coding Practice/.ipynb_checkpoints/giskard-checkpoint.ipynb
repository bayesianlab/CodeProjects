{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd46c5a0",
   "metadata": {},
   "source": [
    "# Here is a time series example\n",
    "This example idea comes from some of my previous research on Bayesian Dynamic Factor Models. The dataset is taken from PennWorld Tables most recent release of data. The data for countries with the highest quality was taken, leaving a dataset on 60 countries. There are observations of each countries GDP, consumption, and investment aggregates. After transforming the data to growth rates and standardizing I perform this analysis. \n",
    "\n",
    "I tried a few things for this ML exercise. \n",
    "- Do a PCA analysis to extract a world factor. The world factor should represent to some degree the level of globalization and the extent to which the economies of the world co-move together. \n",
    "- Use PCA analysis to see if there are significant regional factors. \n",
    "- Use Elastic Net to shrink parameters that are not useful for prediction to 0\n",
    "- Do a large VAR(1) and use Elastic Net to shrink parameters to 0 that are not useful for prediction, and compare this to the standard VAR(1) approach. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "281b75d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.metrics import mean_absolute_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea98db05",
   "metadata": {},
   "outputs": [],
   "source": [
    "world_data = pd.read_csv('kow_standardized.csv', index_col=False, \n",
    "                         header=None)\n",
    "world_data = world_data.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1dcb88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractFactor(data):\n",
    "    cov = np.cov(data)\n",
    "    e, ev = np.linalg.eigh(cov)\n",
    "    idx = np.argsort(e)\n",
    "    ev = ev[:,idx]\n",
    "    components = ev@data\n",
    "    return components.iloc[:,0]\n",
    "\n",
    "wf = extractFactor(world_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d519d996",
   "metadata": {},
   "source": [
    "# Defining the regions\n",
    "These indices are the different regions in the sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9cd96be",
   "metadata": {},
   "outputs": [],
   "source": [
    "north_america = [0,8]\n",
    "oceania = [9,14]\n",
    "latin_america = [15,68]\n",
    "europe = [69,122]\n",
    "africa = [123, 143]\n",
    "asia_minor = [144,161]\n",
    "asia_dev = [162, 179]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a28c9c8",
   "metadata": {},
   "source": [
    "# Extracting regional factors\n",
    "In what follows the PCA method from python is used to extract the \n",
    "seven regional factors. Regions are defined based on the research of Kose, Otrok, Whiteman 2003. The consist of North America, Oceania, Latin America, Europe, Africa, Asia Developing, Asia Developed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8779a73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "na_region = world_data.iloc[:, north_america[0]:north_america[1]]\n",
    "naf = extractFactor(na_region)\n",
    "\n",
    "ocean_region = world_data.iloc[:, oceania[0]:oceania[1]]\n",
    "oceanf = extractFactor(ocean_region)\n",
    "\n",
    "la_region = world_data.iloc[:, latin_america[0]:latin_america[1]]\n",
    "laf = extractFactor(la_region)\n",
    "\n",
    "eur_region = world_data.iloc[:, europe[0]:europe[1]]\n",
    "eurf = extractFactor(eur_region)\n",
    "\n",
    "afr_region = world_data.iloc[:, africa[0]:africa[1]]\n",
    "afrf = extractFactor(afr_region)\n",
    "\n",
    "asm_region = world_data.iloc[:, asia_minor[0]:asia_minor[1]]\n",
    "asmf = extractFactor(asm_region)\n",
    "\n",
    "asd_region = world_data.iloc[:, asia_dev[0]:asia_dev[1]]\n",
    "asdf = extractFactor(asd_region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14c6b71",
   "metadata": {},
   "source": [
    "# Testing Elastic Net CV\n",
    "This runs a quick test to see how Elastic Net CV works. Elastic Net is a compromise between Lasso and Ridge regression (Lasso tends to reject to often and Ridge never quite eliminates everything it should, Elastic Net is a compromise method that has good shrinkage properties while not over compensating by rejecting too much) \n",
    "\n",
    "Elastic Net must be tuned, however, the L1 penalization term and the $\\alpha$ paramter are quite sensitive, changing the results significantly. The cross validation method in Elastic Net CV is useful for tuning these parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29bfaeb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.04250046, 0.15722447, 1.05871679])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.random.seed(100)\n",
    "xtest = pd.DataFrame(np.random.normal(0,1,(100,3)))\n",
    "beta = np.array([1,0,1])\n",
    "eps = pd.DataFrame(np.random.normal(0,1,(100,1)))\n",
    "y = pd.DataFrame(np.matmul(xtest,beta)) + eps\n",
    "encv = ElasticNetCV(l1_ratio=np.linspace(.1,.9,10), eps=.001, \n",
    "                    n_alphas=10, fit_intercept=True, max_iter=5000)\n",
    "encv.fit(xtest,np.ravel(y))\n",
    "\n",
    "encv.coef_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35534d0",
   "metadata": {},
   "source": [
    "# Setup of baseline model and Elastic Net\n",
    "The poitn of this exercise is to see how useful the global factor is for prediction, and to verify or reject the conclusions of a very widely cited paper in the American Economic Review journal by Kose, Otrok and Whiteman 2003. In this paper they claimed that there was a very significant global factor, indicative of a global business cycle. \n",
    "\n",
    "There are significant differences between their methodology and mine own here. They modeled the system as an autoregressive process centered at 0, \n",
    "$$ y_{it} = \\beta_w f_{wt} + \\beta_r f_{rt} + \\beta_c f_{ct} + \\varepsilon_{it} ,$$\n",
    "$$ f_{jt} = f_{jt-1} +\\nu_{jt}, $$\n",
    "where, \n",
    "$$ \\varepsilon_{it} = \\phi_1 \\varepsilon_{it-1} + \\phi_2 \\varepsilon_{it-2} + \\phi_3 \\varepsilon_{it-3} + u_{it}, $$\n",
    "and $ \\nu $ is modeled similarly, as an AR(3) process. Here $u_{it} $ is the unforecastable component, modeled as a stochastic process arising from a $ \\mathcal{N}(0,\\sigma^2) $. Note the indices $t$ range from $1,...,58$, $i = 1, ..., 180$ and j represents either the world, regional, or country factors. \n",
    "\n",
    "In contrast, I will be estimating a far simpler model. There will be no attempt to estimate dynamics of the factor, instead I will take it as an exogeneous process given by the decomposition of the principal components of the covariance matrix given by the system generated between the 180 series in $y_t$. For each $i\\in \\{1,...,180\\} $ a univerate linear model will be estimated with a world factor and a regional factor, \n",
    "$$ y_{it} = \\beta_w f_{wt} + \\beta_r f_{rt} + \\varepsilon_{it} $$ \n",
    "\n",
    "The interest lies in which parameters Elastic Net will shrink out to 0, and what kind of performance increase that will give to the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "daa08003",
   "metadata": {},
   "outputs": [],
   "source": [
    "en = ElasticNet(fit_intercept=True, alpha=1, l1_ratio=.5)\n",
    "base = LinearRegression(fit_intercept=True)\n",
    "c = 0\n",
    "feature_select = pd.DataFrame(data={\"wf\":[], \"r\":[]})\n",
    "tsp = TimeSeriesSplit(2)\n",
    "grid = {'alpha':np.linspace(0.01,1,10), 'l1_ratio':np.linspace(0.01,.99,10)}\n",
    "search = GridSearchCV(en, grid, scoring='neg_mean_absolute_error', )\n",
    "score_comparison = pd.DataFrame(data={\"ElasticNet\":[], \"LinearReg\":[]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff047b9",
   "metadata": {},
   "source": [
    "This code puts together the right regional factors and the world factor. For each equation a univariate regression is run with a corresponding elastic net regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2167b7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "Xna = pd.concat([pd.DataFrame(wf), pd.DataFrame(naf)], axis=1)\n",
    "\n",
    "for i in range(0,9):\n",
    "    print(i)\n",
    "    y = world_data.iloc[:,i]\n",
    "    baseline_score = cross_val_score(base, Xna, y, scoring='neg_mean_absolute_error')\n",
    "    score_comparison.loc[i,'LinearReg'] = np.mean(baseline_score)\n",
    "    res = search.fit(Xna, y)\n",
    "    score_comparison.loc[i,'ElasticNet']= res.best_score_\n",
    "    en = ElasticNet(fit_intercept=True, alpha=res.best_params_['alpha'], \n",
    "                    l1_ratio=res.best_params_['l1_ratio'])\n",
    "    en.fit(Xna,y)    \n",
    "    d={\"wf\":[en.coef_[0]], \"r\":[en.coef_[1]]}\n",
    "    feature_select = pd.concat([feature_select, pd.DataFrame(data=d)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "361d4a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "Xoc = pd.concat([pd.DataFrame(wf), pd.DataFrame(oceanf)], axis=1)\n",
    "for i in range(9,15):\n",
    "    print(i)\n",
    "    y = world_data.iloc[:,i]\n",
    "    baseline_score = cross_val_score(base, Xoc, y, \n",
    "                                     scoring='neg_mean_absolute_error')\n",
    "    score_comparison.loc[i,'LinearReg'] = np.mean(baseline_score)\n",
    "    res = search.fit(Xoc, y)\n",
    "    score_comparison.loc[i,'ElasticNet']= res.best_score_\n",
    "    en = ElasticNet(fit_intercept=True, alpha=res.best_params_['alpha'], \n",
    "                    l1_ratio=res.best_params_['l1_ratio'])\n",
    "    en.fit(Xoc,y)    \n",
    "    d={\"wf\":[en.coef_[0]], \"r\":[en.coef_[1]]}\n",
    "    feature_select = pd.concat([feature_select, pd.DataFrame(data=d)], \n",
    "                               axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f352c172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n"
     ]
    }
   ],
   "source": [
    "Xla = pd.concat([pd.DataFrame(wf), pd.DataFrame(laf)], axis=1)  \n",
    "for i in range(15,69):\n",
    "    print(i)\n",
    "    y = world_data.iloc[:,i]\n",
    "    baseline_score = cross_val_score(base, Xla, y, \n",
    "                                     scoring='neg_mean_absolute_error')\n",
    "    score_comparison.loc[i, 'LinearReg'] = np.mean(baseline_score)\n",
    "    res = search.fit(Xla, y)\n",
    "    score_comparison.loc[i, 'ElasticNet']= res.best_score_\n",
    "    en = ElasticNet(fit_intercept=True, alpha=res.best_params_['alpha'], \n",
    "                    l1_ratio=res.best_params_['l1_ratio'])\n",
    "    en.fit(Xla, y)\n",
    "    d={\"wf\":[en.coef_[0]], \"r\":[en.coef_[1]]}\n",
    "    feature_select = pd.concat([feature_select, pd.DataFrame(data=d)], \n",
    "                               axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85382f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n"
     ]
    }
   ],
   "source": [
    "Xeu = pd.concat([pd.DataFrame(wf), pd.DataFrame(eurf)], axis=1)\n",
    "for i in range(69,123):\n",
    "    print(i)\n",
    "    y = world_data.iloc[:,i]\n",
    "    baseline_score = cross_val_score(base, Xeu, y, \n",
    "                                     scoring='neg_mean_absolute_error')\n",
    "    score_comparison.loc[i, 'LinearReg'] = np.mean(baseline_score)\n",
    "    res = search.fit(Xeu, y)\n",
    "    score_comparison.loc[i, 'ElasticNet']= res.best_score_\n",
    "    en = ElasticNet(fit_intercept=True, alpha=res.best_params_['alpha'], \n",
    "                    l1_ratio=res.best_params_['l1_ratio'])\n",
    "    en.fit(Xeu, y)\n",
    "    d={\"wf\":[en.coef_[0]], \"r\":[en.coef_[1]]}\n",
    "    feature_select = pd.concat([feature_select, pd.DataFrame(data=d)],\n",
    "                               axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97e43c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n"
     ]
    }
   ],
   "source": [
    "Xaf = pd.concat([pd.DataFrame(wf), pd.DataFrame(afrf)], axis=1)\n",
    "for i in range(123,144):\n",
    "    print(i)\n",
    "    y = world_data.iloc[:,i]\n",
    "    baseline_score = cross_val_score(base, Xaf, y, \n",
    "                                     scoring='neg_mean_absolute_error')\n",
    "    score_comparison.loc[i, 'LinearReg'] = np.mean(baseline_score)\n",
    "    res = search.fit(Xaf, y)\n",
    "    score_comparison.loc[i, 'ElasticNet']= res.best_score_\n",
    "    en = ElasticNet(fit_intercept=True, alpha=res.best_params_['alpha'], \n",
    "                    l1_ratio=res.best_params_['l1_ratio'])\n",
    "    en.fit(Xaf, y)\n",
    "    d={\"wf\":[en.coef_[0]], \"r\":[en.coef_[1]]}\n",
    "    feature_select = pd.concat([feature_select, pd.DataFrame(data=d)], \n",
    "                               axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7994c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n"
     ]
    }
   ],
   "source": [
    "Xam = pd.concat([pd.DataFrame(wf), pd.DataFrame(asmf)], axis=1)\n",
    "for i in range(144,162):\n",
    "    print(i)\n",
    "    y = world_data.iloc[:,i]\n",
    "    baseline_score = cross_val_score(base, Xam, y, \n",
    "                                     scoring='neg_mean_absolute_error')\n",
    "    score_comparison.loc[i, 'LinearReg'] = np.mean(baseline_score)\n",
    "    res = search.fit(Xam, y)\n",
    "    score_comparison.loc[i, 'ElasticNet']= res.best_score_\n",
    "    en = ElasticNet(fit_intercept=True, alpha=res.best_params_['alpha'], \n",
    "                    l1_ratio=res.best_params_['l1_ratio'])\n",
    "    en.fit(Xam, y)\n",
    "    d={\"wf\":[en.coef_[0]], \"r\":[en.coef_[1]]}\n",
    "    feature_select = pd.concat([feature_select, pd.DataFrame(data=d)],\n",
    "                               axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32ea58fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n"
     ]
    }
   ],
   "source": [
    "Xad = pd.concat([pd.DataFrame(wf), pd.DataFrame(asdf)], axis=1)\n",
    "for i in range(162,180):\n",
    "    print(i)\n",
    "    y = world_data.iloc[:,i]\n",
    "    baseline_score = cross_val_score(base, Xad, y, \n",
    "                                     scoring='neg_mean_absolute_error')\n",
    "    score_comparison.loc[i, 'LinearReg'] = np.mean(baseline_score)\n",
    "    res = search.fit(Xad, y)\n",
    "    score_comparison.loc[i, 'ElasticNet']= res.best_score_\n",
    "    en = ElasticNet(fit_intercept=True, alpha=res.best_params_['alpha'], \n",
    "                    l1_ratio=res.best_params_['l1_ratio'])\n",
    "    en.fit(Xad, y)\n",
    "    d={\"wf\":[en.coef_[0]], \"r\":[en.coef_[1]]}\n",
    "    feature_select = pd.concat([feature_select, pd.DataFrame(data=d)],\n",
    "                               axis=0)\n",
    "\n",
    "\n",
    "feature_select_bool = abs(feature_select) < .01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69384ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_select_bool.reset_index(drop=True,inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b6d1e5",
   "metadata": {},
   "source": [
    "# Results of EN\n",
    "The results are put together in the pandas Data Frame below. The elastic net shrinks some parameters to zero. In order to make this clearer an arbitrary cutoff was chosen as 0.01 (e.g. but other smaller tolerance levels could be used). Values higher than this are considered to be in the regression. \n",
    "\n",
    "A summary of the key findings are summarized below: \n",
    "- The North American region has a significant factor, none of its parameters were regularized to 0 \n",
    "- The European region was well chosen. The following countries had only one of the series of the three possible shrunk to 0: Finland (GDP), Luxembourg (investment), Norway (consumption), Espana (consumption), Sweden (consumption), Switzerland (GDP), Great Britain (consumtion), \n",
    "- Only one of the countries in Europe could be considered as not fitting into the region that well, which is unsurprisingly Greece. Greece had two series shrunk to 0 (GDP, Cons). However, given that Greece has struggled economically, struggles that have especially come to light since 2010, it is no surprise that Greece does not show much comovement with series in this region. \n",
    "- The world factor explained a large amount of the variance in the various processes. However, for some developing nations it seems that the world factor is not useful for predicting movements in key macro indicators. Costa Rica seems to display no benefit to having the world factor. The Domincan Republic is similar, two of the three series are shrunk to 0. Likewise Guatemala (two of three are 0), Jamaica (two of three 0), Panama (two of three 0), Argentina (two of three 0), Brazil (surprisingly two of three 0), Uruguay (two of three 0), Greece (two of three 0), Senegal (two of three 0)\n",
    "- Apparently, since the nations which are not well explained by the world factor are developing their economies are more detached from global processes unlike their developed neighbors. The region which seems most detached is Latin America. Surprsingly Brazil was detached from the world factor. Being one of the major emerging markets I would have expected it to be tied into movements with the rest of the world. However, despite this status it did not appear to have a strong connection to the rest of the world factor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d771102a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "      <th>146</th>\n",
       "      <th>147</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "      <th>150</th>\n",
       "      <th>151</th>\n",
       "      <th>152</th>\n",
       "      <th>153</th>\n",
       "      <th>154</th>\n",
       "      <th>155</th>\n",
       "      <th>156</th>\n",
       "      <th>157</th>\n",
       "      <th>158</th>\n",
       "      <th>159</th>\n",
       "      <th>160</th>\n",
       "      <th>161</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "      <th>165</th>\n",
       "      <th>166</th>\n",
       "      <th>167</th>\n",
       "      <th>168</th>\n",
       "      <th>169</th>\n",
       "      <th>170</th>\n",
       "      <th>171</th>\n",
       "      <th>172</th>\n",
       "      <th>173</th>\n",
       "      <th>174</th>\n",
       "      <th>175</th>\n",
       "      <th>176</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gdpUSA</td>\n",
       "      <td>conUSA</td>\n",
       "      <td>investUSA</td>\n",
       "      <td>gdpCAN</td>\n",
       "      <td>conCAN</td>\n",
       "      <td>investCAN</td>\n",
       "      <td>gdpMEX</td>\n",
       "      <td>conMEX</td>\n",
       "      <td>investMEX</td>\n",
       "      <td>gdpAUS</td>\n",
       "      <td>conAUS</td>\n",
       "      <td>investAUS</td>\n",
       "      <td>gdpNZL</td>\n",
       "      <td>conNZL</td>\n",
       "      <td>investNZL</td>\n",
       "      <td>gdpCRI</td>\n",
       "      <td>conCRI</td>\n",
       "      <td>investCRI</td>\n",
       "      <td>gdpDOM</td>\n",
       "      <td>conDOM</td>\n",
       "      <td>investDOM</td>\n",
       "      <td>gdpSLV</td>\n",
       "      <td>conSLV</td>\n",
       "      <td>investSLV</td>\n",
       "      <td>gdpGTM</td>\n",
       "      <td>conGTM</td>\n",
       "      <td>investGTM</td>\n",
       "      <td>gdpHND</td>\n",
       "      <td>conHND</td>\n",
       "      <td>investHND</td>\n",
       "      <td>gdpJAM</td>\n",
       "      <td>conJAM</td>\n",
       "      <td>investJAM</td>\n",
       "      <td>gdpPAN</td>\n",
       "      <td>conPAN</td>\n",
       "      <td>investPAN</td>\n",
       "      <td>gdpTTO</td>\n",
       "      <td>conTTO</td>\n",
       "      <td>investTTO</td>\n",
       "      <td>gdpARG</td>\n",
       "      <td>conARG</td>\n",
       "      <td>investARG</td>\n",
       "      <td>gdpBOL</td>\n",
       "      <td>conBOL</td>\n",
       "      <td>investBOL</td>\n",
       "      <td>gdpBRA</td>\n",
       "      <td>conBRA</td>\n",
       "      <td>investBRA</td>\n",
       "      <td>gdpCHL</td>\n",
       "      <td>conCHL</td>\n",
       "      <td>investCHL</td>\n",
       "      <td>gdpCOL</td>\n",
       "      <td>conCOL</td>\n",
       "      <td>investCOL</td>\n",
       "      <td>gdpECU</td>\n",
       "      <td>conECU</td>\n",
       "      <td>investECU</td>\n",
       "      <td>gdpPRY</td>\n",
       "      <td>conPRY</td>\n",
       "      <td>investPRY</td>\n",
       "      <td>gdpPER</td>\n",
       "      <td>conPER</td>\n",
       "      <td>investPER</td>\n",
       "      <td>gdpURY</td>\n",
       "      <td>conURY</td>\n",
       "      <td>investURY</td>\n",
       "      <td>gdpVEN</td>\n",
       "      <td>conVEN</td>\n",
       "      <td>investVEN</td>\n",
       "      <td>gdpFRA</td>\n",
       "      <td>conFRA</td>\n",
       "      <td>investFRA</td>\n",
       "      <td>gdpAUT</td>\n",
       "      <td>conAUT</td>\n",
       "      <td>investAUT</td>\n",
       "      <td>gdpBEL</td>\n",
       "      <td>conBEL</td>\n",
       "      <td>investBEL</td>\n",
       "      <td>gdpDNK</td>\n",
       "      <td>conDNK</td>\n",
       "      <td>investDNK</td>\n",
       "      <td>gdpFIN</td>\n",
       "      <td>conFIN</td>\n",
       "      <td>investFIN</td>\n",
       "      <td>gdpDEU</td>\n",
       "      <td>conDEU</td>\n",
       "      <td>investDEU</td>\n",
       "      <td>gdpGRC</td>\n",
       "      <td>conGRC</td>\n",
       "      <td>investGRC</td>\n",
       "      <td>gdpISL</td>\n",
       "      <td>conISL</td>\n",
       "      <td>investISL</td>\n",
       "      <td>gdpIRL</td>\n",
       "      <td>conIRL</td>\n",
       "      <td>investIRL</td>\n",
       "      <td>gdpITA</td>\n",
       "      <td>conITA</td>\n",
       "      <td>investITA</td>\n",
       "      <td>gdpLUX</td>\n",
       "      <td>conLUX</td>\n",
       "      <td>investLUX</td>\n",
       "      <td>gdpNLD</td>\n",
       "      <td>conNLD</td>\n",
       "      <td>investNLD</td>\n",
       "      <td>gdpNOR</td>\n",
       "      <td>conNOR</td>\n",
       "      <td>investNOR</td>\n",
       "      <td>gdpPRT</td>\n",
       "      <td>conPRT</td>\n",
       "      <td>investPRT</td>\n",
       "      <td>gdpESP</td>\n",
       "      <td>conESP</td>\n",
       "      <td>investESP</td>\n",
       "      <td>gdpSWE</td>\n",
       "      <td>conSWE</td>\n",
       "      <td>investSWE</td>\n",
       "      <td>gdpCHE</td>\n",
       "      <td>conCHE</td>\n",
       "      <td>investCHE</td>\n",
       "      <td>gdpGBR</td>\n",
       "      <td>conGBR</td>\n",
       "      <td>investGBR</td>\n",
       "      <td>gdpCMR</td>\n",
       "      <td>conCMR</td>\n",
       "      <td>investCMR</td>\n",
       "      <td>gdpCIV</td>\n",
       "      <td>conCIV</td>\n",
       "      <td>investCIV</td>\n",
       "      <td>gdpKEN</td>\n",
       "      <td>conKEN</td>\n",
       "      <td>investKEN</td>\n",
       "      <td>gdpMAR</td>\n",
       "      <td>conMAR</td>\n",
       "      <td>investMAR</td>\n",
       "      <td>gdpSEN</td>\n",
       "      <td>conSEN</td>\n",
       "      <td>investSEN</td>\n",
       "      <td>gdpZAF</td>\n",
       "      <td>conZAF</td>\n",
       "      <td>investZAF</td>\n",
       "      <td>gdpZWE</td>\n",
       "      <td>conZWE</td>\n",
       "      <td>investZWE</td>\n",
       "      <td>gdpBGD</td>\n",
       "      <td>conBGD</td>\n",
       "      <td>investBGD</td>\n",
       "      <td>gdpIND</td>\n",
       "      <td>conIND</td>\n",
       "      <td>investIND</td>\n",
       "      <td>gdpIDN</td>\n",
       "      <td>conIDN</td>\n",
       "      <td>investIDN</td>\n",
       "      <td>gdpPAK</td>\n",
       "      <td>conPAK</td>\n",
       "      <td>investPAK</td>\n",
       "      <td>gdpPHL</td>\n",
       "      <td>conPHL</td>\n",
       "      <td>investPHL</td>\n",
       "      <td>gdpLKA</td>\n",
       "      <td>conLKA</td>\n",
       "      <td>investLKA</td>\n",
       "      <td>gdpHKG</td>\n",
       "      <td>conHKG</td>\n",
       "      <td>investHKG</td>\n",
       "      <td>gdpJPN</td>\n",
       "      <td>conJPN</td>\n",
       "      <td>investJPN</td>\n",
       "      <td>gdpKOR</td>\n",
       "      <td>conKOR</td>\n",
       "      <td>investKOR</td>\n",
       "      <td>gdpMYS</td>\n",
       "      <td>conMYS</td>\n",
       "      <td>investMYS</td>\n",
       "      <td>gdpSGP</td>\n",
       "      <td>conSGP</td>\n",
       "      <td>investSGP</td>\n",
       "      <td>gdpTHA</td>\n",
       "      <td>conTHA</td>\n",
       "      <td>investTHA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wf</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0       1          2       3       4          5       6       7    \\\n",
       "1   gdpUSA  conUSA  investUSA  gdpCAN  conCAN  investCAN  gdpMEX  conMEX   \n",
       "wf   False    True       True   False    True       True    True    True   \n",
       "r     True    True       True    True    True       True    True    True   \n",
       "\n",
       "          8       9       10         11      12      13         14      15   \\\n",
       "1   investMEX  gdpAUS  conAUS  investAUS  gdpNZL  conNZL  investNZL  gdpCRI   \n",
       "wf       True    True    True       True    True    True       True   False   \n",
       "r        True    True    True       True    True    True       True    True   \n",
       "\n",
       "       16         17      18      19         20      21      22         23   \\\n",
       "1   conCRI  investCRI  gdpDOM  conDOM  investDOM  gdpSLV  conSLV  investSLV   \n",
       "wf   False      False   False    True      False   False    True       True   \n",
       "r    False      False    True    True       True   False    True       True   \n",
       "\n",
       "       24      25         26      27      28         29      30      31   \\\n",
       "1   gdpGTM  conGTM  investGTM  gdpHND  conHND  investHND  gdpJAM  conJAM   \n",
       "wf    True   False      False    True    True       True   False   False   \n",
       "r     True   False      False   False   False       True   False   False   \n",
       "\n",
       "          32      33      34         35      36      37         38      39   \\\n",
       "1   investJAM  gdpPAN  conPAN  investPAN  gdpTTO  conTTO  investTTO  gdpARG   \n",
       "wf       True   False    True      False    True   False       True   False   \n",
       "r        True   False    True      False    True   False       True   False   \n",
       "\n",
       "       40         41      42      43         44      45      46         47   \\\n",
       "1   conARG  investARG  gdpBOL  conBOL  investBOL  gdpBRA  conBRA  investBRA   \n",
       "wf    True      False    True    True      False    True   False      False   \n",
       "r     True      False    True    True      False    True    True       True   \n",
       "\n",
       "       48      49         50      51      52         53      54      55   \\\n",
       "1   gdpCHL  conCHL  investCHL  gdpCOL  conCOL  investCOL  gdpECU  conECU   \n",
       "wf    True    True      False    True    True       True    True    True   \n",
       "r     True   False      False    True    True       True    True    True   \n",
       "\n",
       "          56      57      58         59      60      61         62      63   \\\n",
       "1   investECU  gdpPRY  conPRY  investPRY  gdpPER  conPER  investPER  gdpURY   \n",
       "wf      False    True   False       True    True   False       True   False   \n",
       "r       False    True    True       True    True   False       True    True   \n",
       "\n",
       "       64         65      66      67         68      69      70         71   \\\n",
       "1   conURY  investURY  gdpVEN  conVEN  investVEN  gdpFRA  conFRA  investFRA   \n",
       "wf   False       True    True    True      False    True    True       True   \n",
       "r    False       True    True   False      False    True    True       True   \n",
       "\n",
       "       72      73         74      75      76         77      78      79   \\\n",
       "1   gdpAUT  conAUT  investAUT  gdpBEL  conBEL  investBEL  gdpDNK  conDNK   \n",
       "wf    True    True       True    True   False       True    True   False   \n",
       "r     True    True       True    True    True       True    True    True   \n",
       "\n",
       "          80      81      82         83      84      85         86      87   \\\n",
       "1   investDNK  gdpFIN  conFIN  investFIN  gdpDEU  conDEU  investDEU  gdpGRC   \n",
       "wf       True    True    True       True    True    True       True   False   \n",
       "r        True   False    True       True    True    True       True   False   \n",
       "\n",
       "       88         89      90      91         92      93      94         95   \\\n",
       "1   conGRC  investGRC  gdpISL  conISL  investISL  gdpIRL  conIRL  investIRL   \n",
       "wf   False       True    True    True       True    True    True       True   \n",
       "r    False       True    True    True       True    True    True       True   \n",
       "\n",
       "       96      97         98      99      100        101     102     103  \\\n",
       "1   gdpITA  conITA  investITA  gdpLUX  conLUX  investLUX  gdpNLD  conNLD   \n",
       "wf    True    True       True   False    True       True   False    True   \n",
       "r     True    True       True    True    True      False    True    True   \n",
       "\n",
       "          104     105     106        107     108     109        110     111  \\\n",
       "1   investNLD  gdpNOR  conNOR  investNOR  gdpPRT  conPRT  investPRT  gdpESP   \n",
       "wf       True    True   False       True   False   False       True   False   \n",
       "r        True    True   False       True    True    True       True   False   \n",
       "\n",
       "       112        113     114     115        116     117     118        119  \\\n",
       "1   conESP  investESP  gdpSWE  conSWE  investSWE  gdpCHE  conCHE  investCHE   \n",
       "wf    True       True    True   False       True   False    True       True   \n",
       "r    False       True    True   False       True   False    True       True   \n",
       "\n",
       "       120     121        122     123     124        125     126     127  \\\n",
       "1   gdpGBR  conGBR  investGBR  gdpCMR  conCMR  investCMR  gdpCIV  conCIV   \n",
       "wf    True    True       True    True    True       True   False    True   \n",
       "r     True   False       True    True    True       True   False    True   \n",
       "\n",
       "          128     129     130        131     132     133        134     135  \\\n",
       "1   investCIV  gdpKEN  conKEN  investKEN  gdpMAR  conMAR  investMAR  gdpSEN   \n",
       "wf       True    True    True      False    True    True       True    True   \n",
       "r        True    True    True       True    True    True       True    True   \n",
       "\n",
       "       136        137     138     139        140     141     142        143  \\\n",
       "1   conSEN  investSEN  gdpZAF  conZAF  investZAF  gdpZWE  conZWE  investZWE   \n",
       "wf   False      False    True    True       True   False    True       True   \n",
       "r     True      False    True    True       True    True    True      False   \n",
       "\n",
       "       144     145        146     147     148        149     150     151  \\\n",
       "1   gdpBGD  conBGD  investBGD  gdpIND  conIND  investIND  gdpIDN  conIDN   \n",
       "wf    True   False       True    True    True       True    True   False   \n",
       "r     True   False       True    True    True       True    True   False   \n",
       "\n",
       "          152     153     154        155     156     157        158     159  \\\n",
       "1   investIDN  gdpPAK  conPAK  investPAK  gdpPHL  conPHL  investPHL  gdpLKA   \n",
       "wf       True    True    True       True    True    True      False    True   \n",
       "r       False    True    True       True    True    True      False    True   \n",
       "\n",
       "       160        161     162     163        164     165     166        167  \\\n",
       "1   conLKA  investLKA  gdpHKG  conHKG  investHKG  gdpJPN  conJPN  investJPN   \n",
       "wf   False       True    True   False       True   False   False       True   \n",
       "r     True       True    True   False      False   False   False       True   \n",
       "\n",
       "       168     169        170     171     172        173     174     175  \\\n",
       "1   gdpKOR  conKOR  investKOR  gdpMYS  conMYS  investMYS  gdpSGP  conSGP   \n",
       "wf    True    True       True    True    True       True    True    True   \n",
       "r     True    True       True    True   False       True    True    True   \n",
       "\n",
       "          176     177     178        179  \n",
       "1   investSGP  gdpTHA  conTHA  investTHA  \n",
       "wf       True    True    True       True  \n",
       "r        True    True    True       True  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.options.display.max_columns = None\n",
    "country_index = pd.read_csv('/home/dillon/GoogleDrive/Datasets/kowindex.csv',\n",
    "           header=None)\n",
    "pd.concat([country_index.iloc[:,1], feature_select_bool], axis=1).transpose()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3e49b2",
   "metadata": {},
   "source": [
    "# Another simple example VAR(1)\n",
    "VAR(1) are a popular forecasting tool for macroeconomic data. However, they quickly become oversaturated with parameters when the number of equations approachs $T$, time. In this case there are 58 observations in $T$ while there are 180 equations. Clearly, this falls into the case where oversaturation could quickly become a problem. The matrix $ (X^TX)^{-1} $ would likely have a numerical issues due to multicolinearly. \n",
    "\n",
    "Elastic net can be used to create a more parsimonious model of this system. Since the world economies exhibit comovement as the results above suggest, a VAR(1) could be a useful model to capture this comovement. The elastic net regularization properties will shrink features that are not needed in the system to 0. \n",
    "\n",
    "For comparison this model is compared to a ols regression using all the features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e5c6765",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3b549df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dillon/miniconda3/envs/test/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.197e-03, tolerance: 5.578e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dillon/miniconda3/envs/test/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.474e-03, tolerance: 5.509e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n"
     ]
    }
   ],
   "source": [
    "Varx = world_data.iloc[0:(world_data.shape[0]-1),:]\n",
    "var_score_comparison = pd.DataFrame(data={\"ElasticNet\":[], \"LinearReg\":[]})\n",
    "for i in range(0,180):\n",
    "    print(i)\n",
    "    y = world_data.iloc[1:,i]\n",
    "    en = ElasticNetCV(l1_ratio=grid['l1_ratio'], eps=.001, n_alphas=10,\n",
    "                      fit_intercept=True,max_iter=3000, cv=tsp)\n",
    "    en.fit(Varx, world_data.iloc[1:,i])\n",
    "    yfit = en.predict(Varx)\n",
    "    var_score_comparison.loc[i, 'ElasticNet'] = -mean_absolute_error(y, yfit)\n",
    "    baseline_scores = cross_val_score(base, Varx, world_data.iloc[1:,i])\n",
    "    var_score_comparison.loc[i, 'LinearReg'] = np.mean(baseline_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e7d98d",
   "metadata": {},
   "source": [
    "The results show that the VAR(1) with shrinkage outperform the simple VAR(1) model with no shrinkage. For some of the equations the results are vastly superior. This shows that overfitting is clearly a problem with such a large system and that shrinkage helps for forecasting these macroeconomic series. In no equation was the standard VAR(1) model better than the EN model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4308ab8a",
   "metadata": {},
   "source": [
    "## Ideas for adaptation in to Giskard's framework\n",
    "\n",
    "Time series is different from other types of data in that the sequential order always matters. Prediction of the future are based on the past actions of the data. In fact, if the data displays no properties to revert to its mean, i.e., is not stationary then prediction is impossible or at the least much more difficult to do. \n",
    "\n",
    "Also, the idea of systems of equations are not typically the types of problems machine or deep learning is focused. Deep learning tries to reduce every problem to univariate processes, but macroeconomic data is often endogeneous. It is questionable how well the assumptions of deep learning perform given the systematic nature of macroeconomic data perform, especially given its dismal performance in comparison to the most simple time series models like ARIMA. I assume that this categorization holds across other disiplines as well and not just in economics. \n",
    "\n",
    "Nevertheless, to deploy a good testing framework for time series the most salient issues facing the time series modeler are the following:\n",
    "- Is the data stationary? \n",
    "- What is the in sample/out of sample performance? \n",
    "- What would happen if the inputs changed?\n",
    "\n",
    "I will address each in the next sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d808884",
   "metadata": {},
   "source": [
    "# Stationarity\n",
    "Stationarity is, of course, incredibly important for trying to predict time series data. If the data never reverts to its mean, but is instead a random walk, then what is the sense in trying to predict this series? \n",
    "\n",
    "# Giskard features to deal with stationarity\n",
    "Users of Giskard may be aware of this problem and simply wish to submit the *data* to Giskard with the identified outcome variable and features. Giskard can then perform unit root tests on the stochastic processes, such as the Augmented Dickey-Fuller test for unit roots, which is considered the go-to test for integrated series. Giskard could then suggest the following options: \n",
    "- Suggest transformations of the data, log differencing, differencing I times, de-trending, or de-seasoning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ada253",
   "metadata": {},
   "source": [
    "Given my example with the Penn World Tables dataset on country level GDP, consumption and investment the unit root  tests can be performed simply with python: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2980eb87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0000\n",
      "0.0025\n",
      "0.4513\n",
      "0.0000\n",
      "0.0330\n",
      "0.5067\n",
      "0.0000\n",
      "0.0000\n",
      "0.5094\n",
      "0.0000\n",
      "0.0022\n",
      "0.3927\n",
      "0.0008\n",
      "0.0000\n",
      "0.0134\n",
      "0.0002\n",
      "0.0000\n",
      "0.0259\n",
      "0.0000\n",
      "0.0000\n",
      "0.0615\n",
      "0.0168\n",
      "0.0018\n",
      "0.0818\n",
      "0.0035\n",
      "0.0393\n",
      "0.0854\n",
      "0.0000\n",
      "0.0000\n",
      "0.1350\n",
      "0.0000\n",
      "0.0000\n",
      "0.1290\n",
      "0.0000\n",
      "0.0000\n",
      "0.1542\n",
      "0.1424\n",
      "0.0568\n",
      "0.1084\n",
      "0.0000\n",
      "0.0000\n",
      "0.1368\n",
      "0.0116\n",
      "0.0000\n",
      "0.1878\n",
      "0.0006\n",
      "0.4268\n",
      "0.7168\n",
      "0.0000\n",
      "0.0001\n",
      "0.1830\n",
      "0.0000\n",
      "0.0000\n",
      "0.1902\n",
      "0.0000\n",
      "0.0000\n",
      "0.5060\n",
      "0.0000\n",
      "0.0060\n",
      "0.4419\n",
      "0.0001\n",
      "0.0000\n",
      "0.0159\n",
      "0.0001\n",
      "0.0109\n",
      "0.0031\n",
      "0.9977\n",
      "0.9882\n",
      "0.7710\n",
      "0.0026\n",
      "0.1970\n",
      "0.5957\n",
      "0.0000\n",
      "0.6119\n",
      "0.8259\n",
      "0.0000\n",
      "0.3498\n",
      "0.4726\n",
      "0.0000\n",
      "0.0283\n",
      "0.6375\n",
      "0.0001\n",
      "0.0124\n",
      "0.4344\n",
      "0.0000\n",
      "0.0073\n",
      "0.0320\n",
      "0.0059\n",
      "0.0382\n",
      "0.8224\n",
      "0.0001\n",
      "0.0000\n",
      "0.0898\n",
      "0.0000\n",
      "0.0063\n",
      "0.0531\n",
      "0.5159\n",
      "0.0314\n",
      "0.8418\n",
      "0.0000\n",
      "0.0001\n",
      "0.0010\n",
      "0.0007\n",
      "0.0604\n",
      "0.5559\n",
      "0.0026\n",
      "0.0000\n",
      "0.1802\n",
      "0.6021\n",
      "0.3019\n",
      "0.8479\n",
      "0.0059\n",
      "0.0239\n",
      "0.4754\n",
      "0.0000\n",
      "0.0003\n",
      "0.4563\n",
      "0.0021\n",
      "0.0584\n",
      "0.1297\n",
      "0.0000\n",
      "0.0002\n",
      "0.4399\n",
      "0.0238\n",
      "0.0240\n",
      "0.2614\n",
      "0.0000\n",
      "0.0000\n",
      "0.2775\n",
      "0.0000\n",
      "0.0000\n",
      "0.1877\n",
      "0.0000\n",
      "0.0008\n",
      "0.0818\n",
      "0.0000\n",
      "0.0000\n",
      "0.8846\n",
      "0.0005\n",
      "0.0008\n",
      "0.4551\n",
      "0.0000\n",
      "0.0000\n",
      "0.0001\n",
      "0.6981\n",
      "0.0058\n",
      "0.1830\n",
      "0.0000\n",
      "0.0059\n",
      "0.2316\n",
      "0.0000\n",
      "0.0001\n",
      "0.0703\n",
      "0.0000\n",
      "0.0000\n",
      "0.0107\n",
      "0.0009\n",
      "0.0150\n",
      "0.0423\n",
      "0.0000\n",
      "0.0004\n",
      "0.0969\n",
      "0.0348\n",
      "0.0003\n",
      "0.2180\n",
      "0.0180\n",
      "0.4081\n",
      "0.0052\n",
      "0.8085\n",
      "0.5807\n",
      "0.2767\n",
      "0.0000\n",
      "0.0000\n",
      "0.0983\n",
      "0.0000\n",
      "0.0011\n",
      "0.7189\n",
      "0.0000\n",
      "0.0000\n",
      "0.6377\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "for i in range(world_data.shape[1]):\n",
    "    pval = float(adfuller(world_data.iloc[:,i])[1])\n",
    "    print(\"{:.4f}\".format(pval))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6a6535",
   "metadata": {},
   "source": [
    "The data have already been log first differenced. Most of the series are showing no evidence of unit root ($H_0 : $ unit root). The series that are showing evidence of unit root I will not transform any more for analysis since overdifferencing is problematic. \n",
    "\n",
    "Solutions for unit root include: \n",
    "- Log first differencing\n",
    "- Differencing\n",
    "- Deseasoning the data with exponential smoothing (Holts method)\n",
    "- Detrending the data with a time trend\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be75bd25",
   "metadata": {},
   "source": [
    "# Example time series plots\n",
    "Time series plots can be inspected for evidence of unit roots or trends:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "507c64a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fecb4c4e3d0>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "T = np.arange(0,100,1)\n",
    "y = 1.5 * T + np.random.normal(0,10,(100))\n",
    "plt.plot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76076b4",
   "metadata": {},
   "source": [
    "Clearly, the data shows a trend which is obvious graphically. There are tests for linear time trends and seasonalitiy, see e.g. Hamilton (1994). A diagnositc result with these tests would be helpful to know which transformation to implement, which Giskard could provide. In summary, the most helpful diagnostics Giskard could give for a modeler who was non-technical but wanted visual inspection would be:\n",
    "- Time series plots of all series\n",
    "- Unit root tests on all series\n",
    "- Tests for seasonality and trends\n",
    "- Proposed solutions for these problems (already covered above)\n",
    "- Levels of autocorrelation and partial autocorrelation observed in the series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995b8c6c",
   "metadata": {},
   "source": [
    "# Time Series models forecasting performance \n",
    "Once a model is submitted to Giskard then one can evaluate its performance similarly to how Giskard already evaluates other models. The methods of validation would need to take into account the sequential nature of time series data and make sure the cross-validation methods being used were not randomizing the sample. The most valuable diagnostics would be its forecasting performance, perhaps under different measures of loss, like mean absolute, mean squared, or median absolute loss. \n",
    "\n",
    "This was done previously above: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f2065c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ElasticNet</th>\n",
       "      <th>LinearReg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.647497</td>\n",
       "      <td>-0.698032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.110000</td>\n",
       "      <td>-0.885935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.067737</td>\n",
       "      <td>-1.359108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.449501</td>\n",
       "      <td>-1.117737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.256992</td>\n",
       "      <td>-2.088626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.008787</td>\n",
       "      <td>-0.105144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.592821</td>\n",
       "      <td>-2.097847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.620764</td>\n",
       "      <td>-2.781272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.372773</td>\n",
       "      <td>-1.895084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.718570</td>\n",
       "      <td>-1.929865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.654879</td>\n",
       "      <td>-3.559830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.065662</td>\n",
       "      <td>-1.762431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.752968</td>\n",
       "      <td>-3.014972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.766178</td>\n",
       "      <td>-2.333775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.176166</td>\n",
       "      <td>-0.201993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.519697</td>\n",
       "      <td>-3.518421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.579495</td>\n",
       "      <td>-2.536804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.174305</td>\n",
       "      <td>-0.609338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.680789</td>\n",
       "      <td>-1.780255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.583095</td>\n",
       "      <td>-3.183568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.256734</td>\n",
       "      <td>-2.142782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.495294</td>\n",
       "      <td>-2.148686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.421563</td>\n",
       "      <td>-3.746750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.050910</td>\n",
       "      <td>-0.869823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.292512</td>\n",
       "      <td>-3.397202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.411682</td>\n",
       "      <td>-1.998767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.519736</td>\n",
       "      <td>-9.693625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.726107</td>\n",
       "      <td>-0.897792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.751919</td>\n",
       "      <td>-2.293553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.129121</td>\n",
       "      <td>-1.441664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>-0.672889</td>\n",
       "      <td>-2.516650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>-0.742891</td>\n",
       "      <td>-4.666098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>-0.219654</td>\n",
       "      <td>-4.573118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>-0.716566</td>\n",
       "      <td>-1.655982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>-0.664643</td>\n",
       "      <td>-11.042149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>-0.012767</td>\n",
       "      <td>-2.926319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>-0.778879</td>\n",
       "      <td>-4.892545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>-0.730735</td>\n",
       "      <td>-5.534942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>-0.040939</td>\n",
       "      <td>-1.460003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>-0.837056</td>\n",
       "      <td>-1.505745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>-0.810112</td>\n",
       "      <td>-1.257207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>-0.043484</td>\n",
       "      <td>-0.330636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>-0.659616</td>\n",
       "      <td>-3.738170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>-0.553046</td>\n",
       "      <td>-2.750517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>-0.371198</td>\n",
       "      <td>-5.745344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>-0.543649</td>\n",
       "      <td>-2.828168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>-0.500017</td>\n",
       "      <td>-2.096167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>-0.204286</td>\n",
       "      <td>-0.425204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>-0.643274</td>\n",
       "      <td>-2.336533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>-0.673316</td>\n",
       "      <td>-1.939233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>-0.589701</td>\n",
       "      <td>-3.035166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>-0.056448</td>\n",
       "      <td>-1.472112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>-0.436617</td>\n",
       "      <td>-1.106157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>-0.136155</td>\n",
       "      <td>-3.675258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>-0.654964</td>\n",
       "      <td>-1.784409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>-0.557858</td>\n",
       "      <td>-2.731451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>-0.175256</td>\n",
       "      <td>-12.116586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>-0.756330</td>\n",
       "      <td>-1.007111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>-0.727642</td>\n",
       "      <td>-2.039187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>-0.289985</td>\n",
       "      <td>-4.381713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>-0.674567</td>\n",
       "      <td>-2.198666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>-0.660009</td>\n",
       "      <td>-2.796729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>-0.178906</td>\n",
       "      <td>0.007353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>-0.779578</td>\n",
       "      <td>-0.675394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>-0.689892</td>\n",
       "      <td>-0.371100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>-0.529403</td>\n",
       "      <td>-0.703540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>-0.301507</td>\n",
       "      <td>-9.039536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>-0.655068</td>\n",
       "      <td>-6.296903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>-0.228321</td>\n",
       "      <td>-1.098334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>-0.358833</td>\n",
       "      <td>-4.843068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>-0.392265</td>\n",
       "      <td>-3.427128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>-0.009171</td>\n",
       "      <td>-14.853526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>-0.407437</td>\n",
       "      <td>-2.753138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>-0.423646</td>\n",
       "      <td>-5.074910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>-0.026221</td>\n",
       "      <td>-4.403426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>-0.370504</td>\n",
       "      <td>-3.018617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>-0.081869</td>\n",
       "      <td>-3.191664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>-0.161919</td>\n",
       "      <td>-3.973655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>-0.663831</td>\n",
       "      <td>-1.968395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>-0.678113</td>\n",
       "      <td>-2.870168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>-0.077095</td>\n",
       "      <td>-2.674146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>-0.520964</td>\n",
       "      <td>-1.870692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>-0.502100</td>\n",
       "      <td>-2.143054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>-0.044300</td>\n",
       "      <td>-0.745569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>-0.459178</td>\n",
       "      <td>-2.300082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>-0.190984</td>\n",
       "      <td>-0.988800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>-0.076595</td>\n",
       "      <td>-1.803213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>-0.487338</td>\n",
       "      <td>-3.001149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>-0.082296</td>\n",
       "      <td>-5.706910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>-0.082711</td>\n",
       "      <td>-8.593042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>-0.547534</td>\n",
       "      <td>-1.978940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>-0.634446</td>\n",
       "      <td>-2.401687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>-0.169380</td>\n",
       "      <td>-0.819180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>-0.712388</td>\n",
       "      <td>-7.879348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>-0.138539</td>\n",
       "      <td>-2.456853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>-0.074305</td>\n",
       "      <td>-9.619198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>-0.383002</td>\n",
       "      <td>-0.864007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>-0.193104</td>\n",
       "      <td>-1.102475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>-0.048476</td>\n",
       "      <td>-6.568100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>-0.204862</td>\n",
       "      <td>-0.660246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>-0.551081</td>\n",
       "      <td>-0.705228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>-0.532749</td>\n",
       "      <td>-0.961536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>-0.259624</td>\n",
       "      <td>-1.443964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>-0.356795</td>\n",
       "      <td>-1.776066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>-0.010231</td>\n",
       "      <td>-3.679780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>-0.074511</td>\n",
       "      <td>-1.637002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>-0.630897</td>\n",
       "      <td>-2.566484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>-0.157838</td>\n",
       "      <td>-0.445733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>-0.500420</td>\n",
       "      <td>-0.656599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>-0.429023</td>\n",
       "      <td>-1.991576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>-0.073356</td>\n",
       "      <td>-3.713287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>-0.345720</td>\n",
       "      <td>-4.453259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>-0.357497</td>\n",
       "      <td>-4.063152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>-0.019306</td>\n",
       "      <td>-5.593555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>-0.453017</td>\n",
       "      <td>-2.994455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>-0.645354</td>\n",
       "      <td>-0.930421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>-0.040935</td>\n",
       "      <td>-3.060831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>-0.583583</td>\n",
       "      <td>-2.812769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>-0.493586</td>\n",
       "      <td>-2.113325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>-0.040536</td>\n",
       "      <td>-2.598537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>-0.702972</td>\n",
       "      <td>-2.076071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>-0.810415</td>\n",
       "      <td>-1.120262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>-0.179910</td>\n",
       "      <td>-2.950852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>-0.654744</td>\n",
       "      <td>-7.869864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>-0.668174</td>\n",
       "      <td>-10.781961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>-0.510055</td>\n",
       "      <td>-3.550568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>-0.791457</td>\n",
       "      <td>-3.575738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>-0.747327</td>\n",
       "      <td>-4.034400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>-0.221714</td>\n",
       "      <td>-6.762362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>-0.724532</td>\n",
       "      <td>-4.199791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>-0.740666</td>\n",
       "      <td>-6.853867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>-0.010686</td>\n",
       "      <td>-11.433047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>-0.698891</td>\n",
       "      <td>-1.635831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>-0.721909</td>\n",
       "      <td>-1.622013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>-0.352557</td>\n",
       "      <td>-8.011364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>-0.584044</td>\n",
       "      <td>-2.490936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>-0.523583</td>\n",
       "      <td>-2.232422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>-0.040275</td>\n",
       "      <td>-8.504966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>-0.005868</td>\n",
       "      <td>-1.520418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>-0.666512</td>\n",
       "      <td>-1.622042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>-0.036938</td>\n",
       "      <td>-2.123389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>-0.689790</td>\n",
       "      <td>-19.898221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>-0.386883</td>\n",
       "      <td>-20.267593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>-0.024763</td>\n",
       "      <td>-2.895680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>-0.425471</td>\n",
       "      <td>-9.090664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>-0.499032</td>\n",
       "      <td>-2.002637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>-0.218818</td>\n",
       "      <td>-19.506300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>-0.590289</td>\n",
       "      <td>-3.563517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>-0.646749</td>\n",
       "      <td>-3.631148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>-0.242897</td>\n",
       "      <td>-2.261539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>-0.574946</td>\n",
       "      <td>-23.501413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>-0.683624</td>\n",
       "      <td>-8.546227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>-0.258119</td>\n",
       "      <td>-25.288477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>-0.795336</td>\n",
       "      <td>-2.706014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>-0.737991</td>\n",
       "      <td>-2.345688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>-0.009855</td>\n",
       "      <td>-3.715250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>-0.424683</td>\n",
       "      <td>-1.813011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>-0.345737</td>\n",
       "      <td>-3.230862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>-0.825188</td>\n",
       "      <td>-7.198620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>-0.729824</td>\n",
       "      <td>-1.793874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>-0.813926</td>\n",
       "      <td>-1.513460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>-0.438117</td>\n",
       "      <td>-0.944060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>-0.483775</td>\n",
       "      <td>-2.922252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>-0.402699</td>\n",
       "      <td>-2.031687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>-0.228336</td>\n",
       "      <td>-13.602822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>-0.418059</td>\n",
       "      <td>-4.992139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>-0.394077</td>\n",
       "      <td>-1.727050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>-0.097129</td>\n",
       "      <td>-3.362208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>-0.412937</td>\n",
       "      <td>-6.617799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>-0.373905</td>\n",
       "      <td>-12.938491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>-0.235552</td>\n",
       "      <td>-31.855019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>-0.560817</td>\n",
       "      <td>-16.386459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>-0.542774</td>\n",
       "      <td>-3.940881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>-0.192807</td>\n",
       "      <td>-5.857220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>-0.444690</td>\n",
       "      <td>-3.579055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>-0.720070</td>\n",
       "      <td>-3.348159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>-0.136463</td>\n",
       "      <td>-0.742114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>-0.451981</td>\n",
       "      <td>-3.503769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>-0.511083</td>\n",
       "      <td>-2.807372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>-0.226176</td>\n",
       "      <td>-12.369052</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ElasticNet  LinearReg\n",
       "0     -0.647497  -0.698032\n",
       "1     -0.110000  -0.885935\n",
       "2     -0.067737  -1.359108\n",
       "3     -0.449501  -1.117737\n",
       "4     -0.256992  -2.088626\n",
       "5     -0.008787  -0.105144\n",
       "6     -0.592821  -2.097847\n",
       "7     -0.620764  -2.781272\n",
       "8     -0.372773  -1.895084\n",
       "9     -0.718570  -1.929865\n",
       "10    -0.654879  -3.559830\n",
       "11    -0.065662  -1.762431\n",
       "12    -0.752968  -3.014972\n",
       "13    -0.766178  -2.333775\n",
       "14    -0.176166  -0.201993\n",
       "15    -0.519697  -3.518421\n",
       "16    -0.579495  -2.536804\n",
       "17    -0.174305  -0.609338\n",
       "18    -0.680789  -1.780255\n",
       "19    -0.583095  -3.183568\n",
       "20    -0.256734  -2.142782\n",
       "21    -0.495294  -2.148686\n",
       "22    -0.421563  -3.746750\n",
       "23    -0.050910  -0.869823\n",
       "24    -0.292512  -3.397202\n",
       "25    -0.411682  -1.998767\n",
       "26    -0.519736  -9.693625\n",
       "27    -0.726107  -0.897792\n",
       "28    -0.751919  -2.293553\n",
       "29    -0.129121  -1.441664\n",
       "30    -0.672889  -2.516650\n",
       "31    -0.742891  -4.666098\n",
       "32    -0.219654  -4.573118\n",
       "33    -0.716566  -1.655982\n",
       "34    -0.664643 -11.042149\n",
       "35    -0.012767  -2.926319\n",
       "36    -0.778879  -4.892545\n",
       "37    -0.730735  -5.534942\n",
       "38    -0.040939  -1.460003\n",
       "39    -0.837056  -1.505745\n",
       "40    -0.810112  -1.257207\n",
       "41    -0.043484  -0.330636\n",
       "42    -0.659616  -3.738170\n",
       "43    -0.553046  -2.750517\n",
       "44    -0.371198  -5.745344\n",
       "45    -0.543649  -2.828168\n",
       "46    -0.500017  -2.096167\n",
       "47    -0.204286  -0.425204\n",
       "48    -0.643274  -2.336533\n",
       "49    -0.673316  -1.939233\n",
       "50    -0.589701  -3.035166\n",
       "51    -0.056448  -1.472112\n",
       "52    -0.436617  -1.106157\n",
       "53    -0.136155  -3.675258\n",
       "54    -0.654964  -1.784409\n",
       "55    -0.557858  -2.731451\n",
       "56    -0.175256 -12.116586\n",
       "57    -0.756330  -1.007111\n",
       "58    -0.727642  -2.039187\n",
       "59    -0.289985  -4.381713\n",
       "60    -0.674567  -2.198666\n",
       "61    -0.660009  -2.796729\n",
       "62    -0.178906   0.007353\n",
       "63    -0.779578  -0.675394\n",
       "64    -0.689892  -0.371100\n",
       "65    -0.529403  -0.703540\n",
       "66    -0.301507  -9.039536\n",
       "67    -0.655068  -6.296903\n",
       "68    -0.228321  -1.098334\n",
       "69    -0.358833  -4.843068\n",
       "70    -0.392265  -3.427128\n",
       "71    -0.009171 -14.853526\n",
       "72    -0.407437  -2.753138\n",
       "73    -0.423646  -5.074910\n",
       "74    -0.026221  -4.403426\n",
       "75    -0.370504  -3.018617\n",
       "76    -0.081869  -3.191664\n",
       "77    -0.161919  -3.973655\n",
       "78    -0.663831  -1.968395\n",
       "79    -0.678113  -2.870168\n",
       "80    -0.077095  -2.674146\n",
       "81    -0.520964  -1.870692\n",
       "82    -0.502100  -2.143054\n",
       "83    -0.044300  -0.745569\n",
       "84    -0.459178  -2.300082\n",
       "85    -0.190984  -0.988800\n",
       "86    -0.076595  -1.803213\n",
       "87    -0.487338  -3.001149\n",
       "88    -0.082296  -5.706910\n",
       "89    -0.082711  -8.593042\n",
       "90    -0.547534  -1.978940\n",
       "91    -0.634446  -2.401687\n",
       "92    -0.169380  -0.819180\n",
       "93    -0.712388  -7.879348\n",
       "94    -0.138539  -2.456853\n",
       "95    -0.074305  -9.619198\n",
       "96    -0.383002  -0.864007\n",
       "97    -0.193104  -1.102475\n",
       "98    -0.048476  -6.568100\n",
       "99    -0.204862  -0.660246\n",
       "100   -0.551081  -0.705228\n",
       "101   -0.532749  -0.961536\n",
       "102   -0.259624  -1.443964\n",
       "103   -0.356795  -1.776066\n",
       "104   -0.010231  -3.679780\n",
       "105   -0.074511  -1.637002\n",
       "106   -0.630897  -2.566484\n",
       "107   -0.157838  -0.445733\n",
       "108   -0.500420  -0.656599\n",
       "109   -0.429023  -1.991576\n",
       "110   -0.073356  -3.713287\n",
       "111   -0.345720  -4.453259\n",
       "112   -0.357497  -4.063152\n",
       "113   -0.019306  -5.593555\n",
       "114   -0.453017  -2.994455\n",
       "115   -0.645354  -0.930421\n",
       "116   -0.040935  -3.060831\n",
       "117   -0.583583  -2.812769\n",
       "118   -0.493586  -2.113325\n",
       "119   -0.040536  -2.598537\n",
       "120   -0.702972  -2.076071\n",
       "121   -0.810415  -1.120262\n",
       "122   -0.179910  -2.950852\n",
       "123   -0.654744  -7.869864\n",
       "124   -0.668174 -10.781961\n",
       "125   -0.510055  -3.550568\n",
       "126   -0.791457  -3.575738\n",
       "127   -0.747327  -4.034400\n",
       "128   -0.221714  -6.762362\n",
       "129   -0.724532  -4.199791\n",
       "130   -0.740666  -6.853867\n",
       "131   -0.010686 -11.433047\n",
       "132   -0.698891  -1.635831\n",
       "133   -0.721909  -1.622013\n",
       "134   -0.352557  -8.011364\n",
       "135   -0.584044  -2.490936\n",
       "136   -0.523583  -2.232422\n",
       "137   -0.040275  -8.504966\n",
       "138   -0.005868  -1.520418\n",
       "139   -0.666512  -1.622042\n",
       "140   -0.036938  -2.123389\n",
       "141   -0.689790 -19.898221\n",
       "142   -0.386883 -20.267593\n",
       "143   -0.024763  -2.895680\n",
       "144   -0.425471  -9.090664\n",
       "145   -0.499032  -2.002637\n",
       "146   -0.218818 -19.506300\n",
       "147   -0.590289  -3.563517\n",
       "148   -0.646749  -3.631148\n",
       "149   -0.242897  -2.261539\n",
       "150   -0.574946 -23.501413\n",
       "151   -0.683624  -8.546227\n",
       "152   -0.258119 -25.288477\n",
       "153   -0.795336  -2.706014\n",
       "154   -0.737991  -2.345688\n",
       "155   -0.009855  -3.715250\n",
       "156   -0.424683  -1.813011\n",
       "157   -0.345737  -3.230862\n",
       "158   -0.825188  -7.198620\n",
       "159   -0.729824  -1.793874\n",
       "160   -0.813926  -1.513460\n",
       "161   -0.438117  -0.944060\n",
       "162   -0.483775  -2.922252\n",
       "163   -0.402699  -2.031687\n",
       "164   -0.228336 -13.602822\n",
       "165   -0.418059  -4.992139\n",
       "166   -0.394077  -1.727050\n",
       "167   -0.097129  -3.362208\n",
       "168   -0.412937  -6.617799\n",
       "169   -0.373905 -12.938491\n",
       "170   -0.235552 -31.855019\n",
       "171   -0.560817 -16.386459\n",
       "172   -0.542774  -3.940881\n",
       "173   -0.192807  -5.857220\n",
       "174   -0.444690  -3.579055\n",
       "175   -0.720070  -3.348159\n",
       "176   -0.136463  -0.742114\n",
       "177   -0.451981  -3.503769\n",
       "178   -0.511083  -2.807372\n",
       "179   -0.226176 -12.369052"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_rows = None\n",
    "var_score_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066cb633",
   "metadata": {},
   "source": [
    "# Time series models perturbation\n",
    "Perturbation in time series models is done using Impulse Response Functions. The impulse responses give the partial derivatives of a change in the system to one equation while holding the others constant and seeing how the dynamics of the entire system respond to say, a one standard deviation shock. \n",
    "\n",
    "It would be most helpful if Giskard implemented all the impulse response functions to time series models that were in a vector autoregressive form. What would be additionally helpful is if the input to the partial derivatives could be manipulated by the user to see the effects of these changes on the system. \n",
    "\n",
    "A simple implementation of this would be simple. In the vase of VAR(1) system above I will take only a subset of the equations for demonstration purposes. Say that we are interested in modeling the US economy with GDP, investment, and consumption growth rates. This is a simple system that can be modeled as a VAR(1) process. \n",
    "\n",
    "It is necessary to impose some sort of restrictions upon the system in order to calculate the shocks. It is in modeling that the structural model has uncorrelated errors. However, the reduced form model has some form of heteroskedasticity. Knowledge of the covariance matrix identifying the heteroskdasticity is necessary to perform any analysis of IRFs. \n",
    "\n",
    "The most common thing done to identify this is the covariance matrix of the structural residiuals. Then a lower cholesky is taken of this matrix and used to calculate the IRFs. Notice that the lower cholelsky necessarily imposes a sense of causality, the first equation being the one that reacts fastest to changes in the system. \n",
    "\n",
    "There are other types of identifying restrictions that can be made to deal with this issue, however, they are more difficult to code and also depend on the researchers preferences and knowledge of the system. \n",
    "\n",
    "For most cases taking the lower cholesky is a good starting point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "20f50cc6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'tran'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [49]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m usa\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgdp\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvest\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcons\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m usay \u001b[38;5;241m=\u001b[39m usa\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m1\u001b[39m:,:]\n\u001b[0;32m----> 6\u001b[0m \u001b[43musay\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtran\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.9/site-packages/pandas/core/generic.py:5575\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   5569\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[1;32m   5570\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[1;32m   5571\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[1;32m   5572\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[1;32m   5573\u001b[0m ):\n\u001b[1;32m   5574\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[0;32m-> 5575\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'tran'"
     ]
    }
   ],
   "source": [
    "usa = world_data.iloc[:,0:3]\n",
    "usa.columns = (\"gdp\", \"invest\", \"cons\")\n",
    "\n",
    "usay = usa.iloc[1:,:]\n",
    "\n",
    "usay.tran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c7d786",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
