\documentclass{article}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{times}
\usepackage{float}
\usepackage{hyperref}

\begin{document}
\SweaveOpts{concordance=TRUE}
\section*{Question 4}
Poisson Likelihood gives a Jeffreys Prior by $ (I(\theta))^{\frac{1}{2}} $
$$
I(\theta) =  V [ \frac{\partial \log f(x|\theta)}{\partial \theta} ]
$$
$$
V \Big[\frac{\partial \log f(x|\theta)}{\partial \theta}\Big] = \frac{\sum_i^N x_i}{\theta} - N =  \frac{\sum_i^N V(x_i)}{\theta^2} = \frac{N}{\theta} 
$$
Jeffreys Prior ($\pi_J(\theta)) $ is 
$$
\Big(\frac{N}{\theta}\Big)^{\frac{1}{2}}
$$
The posterior will be a Gamma $(Ga(\alpha_n, \beta_n))$ $\alpha_n = \frac{1}{2} + \sum_{i=1}^N x_i $ and $\beta_n = 1/N $
$$
\pi(\theta|x) \propto f(x|\theta)\times \pi(\theta) =  \theta^{\sum x_i} e^{N\theta} \times \theta^{-\frac{1}{2}}
$$
$$
\pi(\theta|x) \propto \theta^{\alpha_n} e^{(\theta/\beta_n)}
$$
Where $\alpha_n = \frac{1}{2} + \sum_{i=1}^N x_i $ and $\beta_n = 1/N $.
\\
In this question, $ \sum_{i=1}^N x_i = 196 $  and $ N = 280 $. The Poisson mean is $ 0.70 $. 

\subsection*{Relative Error Loss i)}
Minimize expected posterior loss:
\begin{align*}
\min E_{\theta|X}[L(\theta, a)] = & E \frac{\partial}{\partial a} L(\theta, a) = 0\\ 
 E[ (\frac{a}{\theta^2} - \frac{1}{\theta})] = & \ 0 \\
 a E[\frac{1}{\theta^2}] = & E[\frac{1}{\theta}] \\ 
 a^* = \frac{E \Big[\frac{1}{\theta^2}\Big]  }{E\Big[ \frac{1}{\theta} \Big]}
\end{align*}
The integration was performed in R, 
<<parti>>=
parti <- function(theta,k){
  res <- theta^(-k) * dgamma(theta, 196.5, rate =280)
}
num <-integrate(parti, 0, Inf, k=1)
den <- integrate(parti,0,Inf,k=2)
cat("astar = ", num[[1]]/den[[1]])
@
Which is very close to the Poisson mean, 0.70.
\subsection*{Squared Error Loss ii)}
By Robert Proposition 2.5.1 with squared error loss the Bayes estimator is the posterior expectation,
$$
a^* = E[\theta|X]
$$
Since the  distribution is $Ga(\alpha_n, \beta_n)$ the posterior mean is $\frac{196.5}{280} \approx 0.7018$, no need to use R for this.
\subsection*{Entopy Loss iii)}

\begin{align*}
\min E_{\theta|X} [L(\theta, a)] = & E \frac{\partial}{\partial a} L(\theta, a) = 1 -E \frac{\theta}{a} \\
a^*  =& E[\theta|X] 
\end{align*}
\\
Since the  distribution is $Ga(\alpha_n, \beta_n)$ the posterior mean is $\frac{196.5}{280} \approx 0.7018$

\section*{Part b)}
For the following let $ \phi(\theta) = \frac{r_0}{\theta + r_0} $ %and $ \frac{\partial }{\partial \theta} \phi(\theta) = -\frac{r_0}{ (\theta + r_0)^2 } $
\subsection*{b.1)}

\begin{align*} 
I(\theta) = &  V(\frac{\partial \log(f(x|\phi)}{\partial \phi}) = V \Big[\frac{x}{\phi} - \frac{r_0}{1-\phi} \Big] \\
I(\theta) = & \frac{r_0}{\phi(1-\phi)}
\end{align*}
The Jeffrey's Prior will be $ \sqrt{I(\phi)} = \sqrt{\frac{r_0}{\phi(1-\phi)^2}} $

\subsection*{b.2)} Find the posterior distribution with this prior. The posterior distribution is a Beta distribution, $ Be(\alpha_n, \beta_n) $ with $\alpha_n = n\bar{x} + \frac{1}{2} $ and $\beta_n = nr_0 $,
$$ f(x;\phi) ={ {x + r_0 - 1 }\choose{x}}^N  \phi^{n \bar{x} } (1-\phi)^{n r_0}  $$
$$ \pi(\phi) = \sqrt{I(\phi)} $$
The posterior is then:
\begin{align*}
\pi(\phi|x) \propto & \phi^{\sum x- \frac{1}{2}} (1 - \phi)^{n r_0 - 1} \\
\pi(\phi|x) \propto &\phi^{\alpha_n - 1}(1 - \phi)^{\beta_n - 1}
\end{align*}
This is the kernel of a Beta distribtution with the parameters given above. 
\subsubsection*{b.2.i}
With R one gets:
<<>>=
partb <- function(theta, k){
  res <- theta^(-k) * dbeta(theta, 196.5, 1120)
}
num <- integrate(partb, 0, Inf, k=1)
den <- integrate(partb, 0, Inf, k=2)
cat(num[[1]]/den[[1]])
@
\subsubsection*{b.2.ii-iii}
The posterior mean is 
$$ 
E [\phi|x] = \frac{\alpha_n}{\alpha_n + \beta_n} = \frac{196.5}{1120 + 196.5} \approx 0.15
$$
\newpage

\section*{Question 3}
<<q3, fig=TRUE>>=
Y <- rnorm(10000,0,5)
eY <- exp(Y)
X <- eY/(1+eY)
plot(density(X), main="Density of X")
@
<<q3a, fig=TRUE>>=
plot(hist(X))
@
Show (using R) the two densities are the same:
\begin{enumerate}
\item Code for partition:
<<>>=
weightedBeta <- function(X,cuts){
  Fn <- ecdf(X)
  sto <- Fn(1/cuts)*dbeta(X,1,cuts)
  for(j in 2:cuts){
    jm1 <- j-1
    b <- j/cuts
    a <- jm1/cuts
    beta <- cuts - j + 1
    sto <- sto + ((Fn(b) - Fn(a))*dbeta(X,j,beta))
  }
  sto
}
@
\item Densities are the same as the number of partitions increase,
\begin{figure}[H]
\centering
<<fig=TRUE, height= 2, width=6>>=
par(mfrow=c(1,4))
Z <- weightedBeta(X, 3)
plot(density(Z), main="Density Z")
Z <- weightedBeta(X, 6)
plot(density(Z), main="Density Z")
Z <- weightedBeta(X, 18)
plot(density(Z), main="Density Z")
Z <- weightedBeta(X, 24)
plot(density(Z), main="Density Z")
@
\caption{Partions = 3, 6, 18, 24}
\end{figure}
\end{enumerate}
\newpage 

\section*{Question 6}
\subsection*{a)}
We are to test multiple hypothesis about the paramter $ \mu_i $. If it is significantly different than zero then the null hypothesis $ H_0 $, will be rejected it will be accepted. The code to run the Rjags for this question is here:
<<jags>>=
library(data.table)
library(rjags)
X <-fread('http://www.ics.uci.edu/~mguindan/teaching/stats225/zscores.txt')
p6data <-  list(y = X$V1, N = length(X$V1))
mod <- jags.model("~/Google Drive/CodeProjects/R/p6.bug",
                  data = p6data, n.chains=3, n.adapt = 1000)
res <- coda.samples(mod, var = c("y", "mu", "gamma"), n.iter=3000)
R <- data.frame(as.matrix(res))
@
And the Jags code itself is here:
\begin{verbatim}
model{
p0 ~ dbeta(1,1)
sigma2 ~ dgamma(0.0001,0.0001)
for(i in 1:N){
  y[i] ~ dnorm(mu[i]*gamma[i], tau[i])
  mu[i] ~ dnorm(0, 1)
  gamma[i] ~ dbern(p0)
  tau[i] = 1 / sigma2
}
}
\end{verbatim}
Based on the median model I will track the $ \gamma_i$'s and those that are 1 with sufficiently high posterior probability I will reject the null hypothesis. In doing this problem the $\gamma$ values that I tracked with JAGS had high probability, such that it seemed I would reject the null that $\mu_i$ was zero for all the tests. I believe that there is some sort of problem with my implementation, however, I could not solve this issue. 

\subsection*{b)} For this problem, after reviewing Newton I decided that the best way to attempt this was to try the following code to find the value $\kappa$, 
<<>>=
X <-fread('http://www.ics.uci.edu/~mguindan/teaching/stats225/zscores.txt')
p6data <-  list(y = X$V1, N = length(X$V1))
mod <- jags.model("~/Google Drive/CodeProjects/R/p6.bug", data = p6data, n.chains=3, n.adapt = 1000)
res <- coda.samples(mod, var = c("y", "mu", "gamma"), n.iter=3000)
R <- data.frame(as.matrix(res))
sto <- numeric(200)
D <- dim(R)
for(i in 1:200){
  sto[i] <- sum(R[,i])/D[1]
}
sort(sto)
sort(1 - sto)
plot(sto, col="blue")
fdr <- function(k, pi, level){
  lpi <- length(pi)
  indicator <- (pi < k)
  betag <- sum( (1-pi) %*% indicator )
  den <- sum(indicator)
  res <- (betag / den) - level
  res
}
optimize(fdr, interval=c(min(sto),max(sto)), tol=.0001, pi=sto, level=.05)
@
The best  I could get was to find a $ \kappa $ equal to 0.96 using this method. The code above is trying to find a the largest $ \kappa $ so that the above was satisfied, but I realize in typing this assignment up that this method is not adequate to solve this problem. 

\end{document}